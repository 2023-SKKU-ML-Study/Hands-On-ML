{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8683742-c560-4fe2-9396-ef89a286429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4758dbe-cbd6-475f-95c5-fba0ffd5a23e",
   "metadata": {},
   "source": [
    "- 텐서플로 데이터 API는 대규모 데이터셋을 효율적으로 로드하고 전처리할 수 있도록 한다.\n",
    "- 텐서플로가 멀티스레딩, 큐, 배치, 프리페치 같은 상세한 사항을 모두 대신 처리해준다.\n",
    "- 또한 데이터 API는 tf.keras와도 잘 동작한다.\n",
    "\n",
    "이 장에서는 데이터 API, TFRecord 포맷을 다루고 사용자 정의 전처리 층을 만드는 방법과 표준 케라스 전처리 층을 사용하는 법을 다룬다.\n",
    "\n",
    "# 1. 데이터 API\n",
    "- from_tensor_slices() 함수는 텐서를 받아 X의 각 원소가 아이템으로 표현되는 tf.data.Dataset을 만든다.<br>\n",
    "    - 즉 데이터셋은 0,1,2,,9에 해당하는 10개의 아이템을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7acbe1-684e-4474-b062-eaf98d8d8372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10) # 데이터 텐서\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X) # 메모리 전체에서 데이터셋 생성\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ae75ce-01cc-4890-92f2-8cb50d08a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11ede7f1-a699-48cb-9a3e-4ed083c0536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "# make dict\n",
    "X_neated = {\"a\": ([1,2,3],[4,5,6]), \"b\":[7,8,9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_neated)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bd713-8c6f-4c11-b2af-2e5f0cea4975",
   "metadata": {},
   "source": [
    "## 1. 연쇄변환\n",
    "데이터셋이 준비되면 변환 메서드를 호출하여 여러 종류의 변환을 수행할 수 있다.\n",
    "- repeat(x) 메서드: 반복횟수. 원본 데이터셋의 아이템을 x차례 반복하는 새로운 데이터셋 반환\n",
    "- batch(x) 메서드: 배치크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dab0204-be36-4cd5-9bf5-30df0d22cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "dataset_dr = dataset.repeat(3).batch(7, drop_remainder=True) # drop_remainder=True로 하면 남은 [8 9]는 drop\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114a0c5-9fa4-4b9c-99d9-84a5b354cc6e",
   "metadata": {},
   "source": [
    "map으로 데이터 변환<br>\n",
    "- num_parallel_calls 매개변수로 여러 스레드로 나누어 속도를 높일 수 있음.\n",
    "- map() 메서드에 전달하는 함수는 텐서플로 함수로 변환 가능해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "557e8d32-7cce-435f-a01c-5cb93032a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 각 아이템에 *2 해주기\n",
    "dataset = dataset.map(lambda x:x * 2) # x는 배치\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bb42b5f-bf29-466a-a982-74c532182dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# reduce_sum : 텐서의 모든 성분의 총합을 계산하는 함수\n",
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c3c383cb-1200-4ba4-8547-83ef66bf2938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 위의 dataset 중에서 2개만 보고싶을 때\n",
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2a0c8-acbd-418d-84c5-7c941d006669",
   "metadata": {},
   "source": [
    "## 2. Shuffling the data\n",
    "경사 하강법은 훈련셋에 있는 샘플이 독립적이고 동일한 분포일 때 최고 성능을 발휘한다. shuffle() 메서드로 간단하게 샘플을 섞어 데이터 분포를 적절하게 만들 수 있다.<br>\n",
    "- shuffle() 메서드<br>\n",
    "  [동작순서]<br>\n",
    "    1. 먼저 원본 데이터셋의 처음 아이템은 buffer_size 개수만큼 추출하여 버퍼에 채운다.\n",
    "    2. 그다음 새로운 아이템이 요청되면 이 버퍼에서 비워진 버퍼를 채운다.\n",
    "    3. 원본 데이터셋의 모든 아이템이 사용될 때까지 반복된다.\n",
    "    4. 그다음엔 버퍼가 비워질 때까지 계속라여 랜덤하게 아이쳄을 반환한다.\n",
    "  - 이 메서드를 사용하려면 버퍼 크기를 지정해야 한다. 메모리를 넘지 않는 선에서 버퍼 크기를 충분히 크게 하는 것이 중요하다.<br>\n",
    " \n",
    "⬇︎ 정수 0에서 9까지 세 번 반복된 데이터셋을 만든 다음, 버퍼 크기 4, 랜덤 시드 42를 사용하여 셔플링하고, 배치 크기 7로 나누어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6f63c55-0d56-4a0d-820a-db11b74bd316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d5c29-c4f3-413e-98c9-9d152c4fa8b5",
   "metadata": {},
   "source": [
    "❗️셔플된 데이터셋이 repeat() 메서드를 호출하면 기본적으로 반복마다 새로운 순서를 생성한다.<br> 하지만 반복마다 동일한 순서를 사용해야 한다면 repeat() 메서드에 reshuffle_each_iteration=False를 지정해야 한다.\n",
    "### 여러 파일에서 한 줄씩 번갈아 읽기\n",
    "⬇︎ california_housing dataset 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68dbde6c-281f-4167-b0c1-024a406fc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b6362dc5-2545-48e7-968e-cfba706e674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 20개 파일로 분할하여 csv로 저장\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
    "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename_format = \"my_{}_{:02d}.csv\"\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    chunks = np.array_split(np.arange(m), n_parts)\n",
    "    for file_idx, row_indices in enumerate(chunks):\n",
    "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(str(part_csv))\n",
    "        with open(part_csv, \"w\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = ','.join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0343c6b5-27c3-406c-8b0c-4f08e97dd23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n",
      "2.0549,13.0,5.312457454050374,1.0850918992511913,3297.0,2.2443839346494214,33.93,-116.93,0.956\n",
      "2.9583,50.0,5.380281690140845,1.1173708920187793,579.0,2.7183098591549295,33.98,-118.06,1.726\n",
      "3.52,23.0,4.698216735253772,1.0342935528120714,2202.0,3.0205761316872426,34.14,-118.01,1.873\n",
      "2.7188,32.0,5.511627906976744,1.067829457364341,1337.0,2.5910852713178296,34.94,-120.42,1.337\n",
      "2.6563,26.0,4.29489291598023,1.1235584843492588,1401.0,2.3080724876441514,37.68,-122.08,1.841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위의 csv 파일 중 하나의 처음 몇 줄 살펴보기\n",
    "print(\"\".join(open(train_filepaths[0]).readlines()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e4507546-607b-4cef-baae-50ebc45eef8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ab563-9ffc-4c85-b4cb-8dbe3348f05e",
   "metadata": {},
   "source": [
    "### 입력 파이프라인 구축\n",
    "기본적으로 list_files() 함수는 파일 경로를 섞은 데이터셋을 반환한다. 이를 원하지 않는다면 shuffle=False로 지정하면 된다. 하지만 일반적으로 shuffle을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4e672c6c-a900-4b86-8d22-2b61a5b1a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abe521ab-1cb5-41d1-9a10-6371fc093543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# filepath가 섞여있음을 보여줌\n",
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f0d96aea-23c2-43bf-91ed-50759d782d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skip(1): 열 이름은 스킵\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421b5b4-364f-4757-870a-6b9adfea2a01",
   "metadata": {},
   "source": [
    "interleave() 메서드는 filepath_dataset에 있는 다섯 개의 파일 경로에서 데이터를 읽는 데이터셋을 만든다.<br>\n",
    "이 메서드에 전달한 lambda 함수는 각 파일에 대해 호출하여 새로운 데이터셋(위의 경우 TextLineDataset)을 만들 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a869d68-5312-42d8-936f-31617ba5047b",
   "metadata": {},
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bef1ed90-d896-41f5-8646-34093a3b7af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\n",
      "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.0217,22.0,4.983870967741935,1.1008064516129032,615.0,2.4798387096774195,38.76,-120.6,1.069'\n",
      "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91f4d5-c48b-4446-9f27-490d0b3b8325",
   "metadata": {},
   "source": [
    "## 3. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b6909f0-0940-448b-ad66-996d508e76b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "09afb90e-48de-4838-8944-86e6b065b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = scaler.mean_, scaler.scale_\n",
    "n_inputs = 8\n",
    "\n",
    "# csv한 라인을 받아 파싱\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs) # io.decode_csv() 함수로 파싱: 열마다 한 개씩 스칼라 텐서의 리스트를 반환\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "# \n",
    "def preprocess(line):\n",
    "    x, y, = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y # 스케일 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2ed547c4-9dbd-4508-94d5-7ccc35645574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       "array([-0.8936168 ,  0.9789995 , -0.6810549 , -0.07264194, -0.5669866 ,\n",
       "       -0.39212456, -1.3522334 ,  1.2316071 ], dtype=float32)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a71a7c-c5dd-4bc6-9458-389aa86c2102",
   "metadata": {},
   "source": [
    "## 4. 데이터 적재와 전처리를 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee266b5-742d-4169-8324-bd15c5be4fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
